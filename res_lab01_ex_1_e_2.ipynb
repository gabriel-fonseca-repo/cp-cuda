{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOpRT9Yf8IJRoNYSzn1zFuG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZShBMckeq_oJ",
        "outputId": "4a186b59-805d-45ef-ac82-71dec3c5cd63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-ws37emcl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-ws37emcl\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4294 sha256=f78af9a8412d8f02a834c6136028cd918ca5757a585370301cd0e97aad926b43\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gebrey90/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2WOO3sLtxxK",
        "outputId": "e6d62e2e-739e-4385-ac06-be19692dc1cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Código do exercício 1."
      ],
      "metadata": {
        "id": "fQuJxRhuwyZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "void cpu_helloworld() {\n",
        "    printf(\"Alô da CPU!\\n\");\n",
        "}\n",
        "\n",
        "__global__ void gpu_helloworld() {\n",
        "    int threadId = threadIdx.x;\n",
        "    printf(\"Alô da GPU! Meu threadId eh %d\\n\", threadId);\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "    dim3 grid(1);\n",
        "    dim3 block(32);\n",
        "\n",
        "    cpu_helloworld();\n",
        "\n",
        "    gpu_helloworld<<<grid, block>>>();\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN5kCJJ-radj",
        "outputId": "597844a9-8107-47d4-dc6c-219f74d27fc6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alô da CPU!\n",
            "Alô da GPU! Meu threadId eh 0\n",
            "Alô da GPU! Meu threadId eh 1\n",
            "Alô da GPU! Meu threadId eh 2\n",
            "Alô da GPU! Meu threadId eh 3\n",
            "Alô da GPU! Meu threadId eh 4\n",
            "Alô da GPU! Meu threadId eh 5\n",
            "Alô da GPU! Meu threadId eh 6\n",
            "Alô da GPU! Meu threadId eh 7\n",
            "Alô da GPU! Meu threadId eh 8\n",
            "Alô da GPU! Meu threadId eh 9\n",
            "Alô da GPU! Meu threadId eh 10\n",
            "Alô da GPU! Meu threadId eh 11\n",
            "Alô da GPU! Meu threadId eh 12\n",
            "Alô da GPU! Meu threadId eh 13\n",
            "Alô da GPU! Meu threadId eh 14\n",
            "Alô da GPU! Meu threadId eh 15\n",
            "Alô da GPU! Meu threadId eh 16\n",
            "Alô da GPU! Meu threadId eh 17\n",
            "Alô da GPU! Meu threadId eh 18\n",
            "Alô da GPU! Meu threadId eh 19\n",
            "Alô da GPU! Meu threadId eh 20\n",
            "Alô da GPU! Meu threadId eh 21\n",
            "Alô da GPU! Meu threadId eh 22\n",
            "Alô da GPU! Meu threadId eh 23\n",
            "Alô da GPU! Meu threadId eh 24\n",
            "Alô da GPU! Meu threadId eh 25\n",
            "Alô da GPU! Meu threadId eh 26\n",
            "Alô da GPU! Meu threadId eh 27\n",
            "Alô da GPU! Meu threadId eh 28\n",
            "Alô da GPU! Meu threadId eh 29\n",
            "Alô da GPU! Meu threadId eh 30\n",
            "Alô da GPU! Meu threadId eh 31\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Código do exercício 2."
      ],
      "metadata": {
        "id": "iw8jOONow22I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include <math.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <sys/time.h>\n",
        "\n",
        "#define BLOCK_SIZE 64\n",
        "#define ARRAY_SIZE 128000\n",
        "\n",
        "typedef struct timeval tval;\n",
        "\n",
        "float generate_hash(int n, float *y) {\n",
        "    float hash = 0.0f;\n",
        "\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        hash += y[i];\n",
        "    }\n",
        "\n",
        "    return hash;\n",
        "}\n",
        "\n",
        "double get_elapsed(tval t0, tval t1) {\n",
        "    return (double)(t1.tv_sec - t0.tv_sec) * 1000.0L +\n",
        "           (double)(t1.tv_usec - t0.tv_usec) / 1000.0L;\n",
        "}\n",
        "\n",
        "void cpu_saxpy(int n, float a, float *x, float *y) {\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        y[i] = a * x[i] + y[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "// TO-DO #2.1\n",
        "// Declarando e implementando o kernel CUDA executará na GPU.\n",
        "__global__ void gpu_saxpy(int n, float a, float *x, float *y) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (tid < n) {\n",
        "        y[tid] = a * x[tid] + y[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "    float a = 0.0f;\n",
        "    float *x = NULL;\n",
        "    float *y = NULL;\n",
        "    float error = 0.0f;\n",
        "\n",
        "    // Verifique se a contante foi fornecida\n",
        "    // if (argc != 2) {\n",
        "    //     fprintf(stderr, \"Erro: A constante está faltando!\\n\");\n",
        "    //     return -1;\n",
        "    // }\n",
        "\n",
        "    // Código original do problema SAXPY\n",
        "    a = 10;\n",
        "    x = (float *)malloc(sizeof(float) * ARRAY_SIZE);\n",
        "    y = (float *)malloc(sizeof(float) * ARRAY_SIZE);\n",
        "    for (int i = 0; i < ARRAY_SIZE; i++) {\n",
        "        x[i] = 0.1f;\n",
        "        y[i] = 0.2f;\n",
        "    }\n",
        "\n",
        "    // TO-DO #2.2\n",
        "    // Definindo a distribuição das threads, em termos da dimensão da grade\n",
        "    // (grid) e da dimensão de cada bloco (de threads) dentro da grade.\n",
        "    dim3 grid((ARRAY_SIZE + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "    dim3 block(BLOCK_SIZE);\n",
        "\n",
        "    // TO-DO #2.3.1\n",
        "    // Declarando e definindo a memória GPU necessária para executar o kernel\n",
        "    // CUDA.\n",
        "    float *d_x, *d_y;\n",
        "    cudaMalloc((void **)&d_x, sizeof(float) * ARRAY_SIZE);\n",
        "    cudaMalloc((void **)&d_y, sizeof(float) * ARRAY_SIZE);\n",
        "\n",
        "    // TO-DO #2.3.2\n",
        "    // Transferindo os dados do host para a GPU.\n",
        "    cudaMemcpy(d_x, x, sizeof(float) * ARRAY_SIZE, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_y, y, sizeof(float) * ARRAY_SIZE, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Variáveis para métricas de performance/tempo de execução da CPU.\n",
        "    tval t_start, t_end;\n",
        "    gettimeofday(&t_start, NULL);\n",
        "    // Execução na CPU para fins de comparação\n",
        "    cpu_saxpy(ARRAY_SIZE, a, x, y);\n",
        "    gettimeofday(&t_end, NULL);\n",
        "    double elapsed_cpu = get_elapsed(t_start, t_end);\n",
        "    error = generate_hash(ARRAY_SIZE, y);\n",
        "\n",
        "    printf(\"Execução na CPU terminada. Taxa de erro = %.6f.\\n\", error);\n",
        "    printf(\"Tempo de execução na CPU: %.6f ms.\\n\", elapsed_cpu);\n",
        "\n",
        "    // Variáveis para métricas de performance/tempo de execução da GPU.\n",
        "    tval t_start_gpu, t_end_gpu;\n",
        "    gettimeofday(&t_start_gpu, NULL);\n",
        "\n",
        "    // TO-DO #2.4\n",
        "    // Execute o kernel CUDA com os parâmetros correspondentes.\n",
        "    gpu_saxpy<<<grid, block>>>(ARRAY_SIZE, a, d_x, d_y);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    gettimeofday(&t_end_gpu, NULL);\n",
        "    double elapsed_gpu = get_elapsed(t_start_gpu, t_end_gpu);\n",
        "\n",
        "    // TO-DO #2.5.1\n",
        "    // Transferindo os resultados da GPU para o host.\n",
        "    cudaMemcpy(y, d_y, sizeof(float) * ARRAY_SIZE, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    error = fabsf(error - generate_hash(ARRAY_SIZE, y));\n",
        "\n",
        "    printf(\"Execução na GPU terminada. Taxa de erro = %.6f.\\n\", error);\n",
        "    printf(\"Tempo de execução na GPU: %.6f ms.\\n\", elapsed_gpu);\n",
        "\n",
        "    if (error > 0.0001f) {\n",
        "        fprintf(stderr, \"Erro: a solução está incorreta!\\n\");\n",
        "    }\n",
        "\n",
        "    // Gerenciamento de memória.\n",
        "    free(x);\n",
        "    free(y);\n",
        "    cudaFree(d_x);\n",
        "    cudaFree(d_y);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxOyrIpMyqBU",
        "outputId": "aa7ad0e4-a3d6-470b-cf43-fd057d53afe2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execução na CPU terminada. Taxa de erro = 153799.109375.\n",
            "Tempo de execução na CPU: 0.329000 ms.\n",
            "Execução na GPU terminada. Taxa de erro = 0.000000.\n",
            "Tempo de execução na GPU: 0.054000 ms\n",
            "\n"
          ]
        }
      ]
    }
  ]
}